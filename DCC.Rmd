---
title: "Cluster Computing and DCC Basics"
author: "Vittorio Orlandi<br>Alessandro Zito (TA of the Year 2021)"
output: 
  ioslides_presentation:
    smaller: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

## Why Use the DCC?

- Jobs that are computationally infeasible on your local machine
- Even if not: small amount of extra work to get e.g. 50x speedup
- What about Knight and Rook? 
    - DCC has more resources
    - Good to learn how to navigate in a Linux environment 

## Getting Started
- Log in (from command line of local machine): \
`ssh netID@dcc-login.oit.duke.edu`
- Go into your directory: `cd netID`
    - If it doesn't exist, create it: `mkdir netID` 
- Clone the repo you'll need for this lab: \
`git clone https://github.com/alessandrozito/STA723_ClusterOverview_Spring2022.git`
- See what's there (`cd` and `ls`)

## Text Editors
- Can modify files locally and then transfer to cluster (later), but useful to know how to modify files using a text editor
- Most common: vim, emacs, nano

## `vim` 
- Text editor
- Annoying at first, but useful 
- Two primary modes:
    - Insert mode: used to write (insert) and delete (not strictly recommended) text
    - Normal mode: used for everything else: moving around, search and replace, deleting, etc.
- .vimrc: file for customizing vim
    

## `vim` Basics
| Command | Function                   | Remember                                                                       |
|-----------------------------|----------------------------|---------------------------------|
| vim \<file\>                        | Open a file with vim          |                                                                    |
| i,    \<esc\>                        | Enter, exit (escape) insert mode          | i for insert, \<esc\> for escape.                                                                   |
| h, j, k, l                  | Move left, down, right, up | h on the left<br>j for gi√π<br>k for climb<br>l on the right                    |
| \<line_num\>G               | Go to line <line_num>      | G for Go!                                                                      |
| \<Ctrl-Up\/Down\>           | Jump up, down              |                                                                                |
| dw, dd                      | Delete word, delete line   | **D**elete **w**ord                                                            |
| 0, \$                       | Go to start, end of a line | 0 is where you start counting<br>\$ is the last reason you decided to do a PhD |
| :w, :q, :wq                 | Save, Quit, Save and Quit  | w for write (to file), q for quit                                              |

## Modify Files 
- Set simulation parameters
- Would like to run for various `epsilon` in parallel. How? 

## Array Jobs
- Run *the same script* `k` many times
- Only difference: an environmental variable used to save results (here, `slurm_id`)
- Two options: 
    - Ignore `slurm_id`, treat different runs identically. E.g. Run a method on fifty identically simulated datasets
    - Use `slurm_id` to differentiate jobs. E.g. each job computes a marginal dependence curve for a different covariate.
    - Here, each job will correspond to a different `epsilon`.
- In either case, *make sure to save simulations separately* 

## Running Code: Batch Scripts
- `.sh` file used to run your code 
- Customize behavior using flags specified by `#SBATCH`
    - `--job-name`: Helps identify jobs you have running 
    - `--mail-user`: email address for job notifications
    - `--mail-type`: what to notify about
    - `--partition`: where the machines to run your jobs come from 
    - `--cpus-per-task`: number of processors *per* array job 
    - `--array`: specify array jobs to run 
- Then command to run script, e.g. `R CMD BATCH`

## Running and Monitoring Jobs
- Run your job with `sbatch`: 
    - Here, `sbatch bandits.sh`
- Check on its status with `squeue -u netID`
- When finished, time to aggregate output

## Aggregating Job Output
- Running an array job will result in many different output files. 
- Typically, want to combine them in some way, e.g: 
    - Get average value across simulations
    - Combine into a data frame 
- Fill in `agg.R` appropriately

## Aggregating Job Output 
- `module load R` to make R accessible, then `R` to start an R session. 
- Inspect job output and then run as usual: `source('agg.R')`. 
- However: **don't typically use R on a login node**
    - Alternative to manually running via R: another job script. 
        - Advantages: 1. Don't worry about memory on login nodes; 2. *Job dependencies*
    - Or interactive session: `srun --pty bash -i` \
    (easier to remember `history | grep srun`). 
  
## File Transfer

*From your local computer...*

Push a single file to DCC using `scp` (remember `history | grep scp`)

```{bash, eval = FALSE}
scp <path_to_local_file> netID@dcc-login.oit.duke.edu:~/
```

Reverse the path order to pull files from DCC.

Similarly, by adding the `-r` flag, you can push/pull directories. For example, to push a directory `dir1/`

```{bash, eval = FALSE}
scp -r dir1/ netID@dcc-login.oit.duke.edu:~/
```

Make a plot. Which epsilon worked best? 

## Wrapping Up 

- DCC lets you run more jobs faster than you can locally
- Getting used to SLURM, Linux, text editing without a mouse, etc. *takes time but is very worth it in the long run*. 
- Additional resources:
    - [SLURM documentation](https://slurm.schedmd.com/documentation.html)
    - [DCC User Guide](https://oit-rc.pages.oit.duke.edu/rcsupportdocs/dcc/)
    - Older students :)

##  {.centered}
\
\
\
\

<font size="10">Thank You!</font>